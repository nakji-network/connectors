// Code generated by connectorgen.
// go run ./connectors/source/uniswapv3/cmd/uniswapv3_backfill/main.go --fromBlock 11945500 --toBlock 11945502
package main

import (
	"context"
	"encoding/json"
	"math/big"
	"net/http"
	"net/url"
	"time"

	"blep.ai/data/chain/ethereum/ethclient"
	"blep.ai/data/config"
	"blep.ai/data/connectors/source/uniswapv3"
	"blep.ai/data/database"
	"github.com/nakji-network/connector/kafkautils"

	geth "github.com/ethereum/go-ethereum"
	ethcommon "github.com/ethereum/go-ethereum/common"
	ethtypes "github.com/ethereum/go-ethereum/core/types"
	"github.com/rs/zerolog/log"
	"github.com/spf13/pflag"

	_ "net/http/pprof"
)

type Status struct {
	Step            int
	EventsProcessed map[string]uint64
	CurrentBlock    uint64
	Errors          []error
	FromBlock       int64
	ToBlock         int64
}

func main() {
	// Memory profiling
	go http.ListenAndServe(":6060", nil)

	// Load config in here to support flags
	pflag.Int64("fromBlock", 10000835, "backfill from this block")
	pflag.Int64("toBlock", 12000000, "backfill to this block")
	pflag.Int64("blockChunk", 3000, "get 100 blocks at a time to prevent timeouts")
	pflag.Int64P("maxConnections", "c", 10, "max simultaneous connections")
	pflag.Bool("upsert", true, "uses batch instead of COPY FROM when existing entries might exist")
	pflag.Int("dbBatch", 3000, "batch db writes by x items")
	pflag.Int64("processPairConcurrency", 4096, "max goroutines for pairProcessLog")

	var conf = config.InitConfig()

	conf.SetDefault("uniswapv3.factoryAddress", "0x1F98431c8aD98523631AE4a59f267346ea31F984")

	// get status of this long running status
	status := Status{
		FromBlock: conf.GetInt64("fromBlock"),
		ToBlock:   conf.GetInt64("toBlock"),
	}
	http.HandleFunc("/status", func(w http.ResponseWriter, req *http.Request) {
		js, err := json.Marshal(status)
		if err != nil {
			http.Error(w, err.Error(), http.StatusInternalServerError)
			return
		}
		w.Header().Set("Content-Type", "application/json")
		w.Write(js)
	})
	go http.ListenAndServe(":8080", nil)

	// Load Topic registry
	kafkautils.TopicTypeRegistry.Load(uniswapv3.TopicTypes)

	rpcUrls := []string{(&url.URL{
		Scheme: conf.GetString("ethereum.archival.scheme"),
		User:   url.UserPassword(conf.GetString("ethereum.archival.username"), conf.GetString("ethereum.archival.password")),
		Host:   conf.GetString("ethereum.archival.host"),
		Path:   conf.GetString("ethereum.archival.path"),
	}).String(),
		(&url.URL{ // hacky to get past wss error
			Scheme: "wss",
			User:   url.UserPassword(conf.GetString("ethereum.archival.username"), conf.GetString("ethereum.archival.password")),
			Host:   conf.GetString("ethereum.archival.host"),
			Path:   conf.GetString("ethereum.archival.path"),
		}).String()}

	ethClientPool, err := ethclient.DialPoolContext(context.Background(), rpcUrls)
	if err != nil {
		log.Fatal().Err(err).Msg("Ethereum RPC connection error")
	}

	// Init historical db
	db, err := database.New(conf.GetString("timescaledb.connection"))
	if err != nil {
		log.Fatal().Err(err).Str("dsn", conf.GetString("timescaledb.connection")).Msg("Timescaledb connection failed")
	}
	defer db.Close()

	factoryAddress := ethcommon.HexToAddress(conf.GetString("uniswapv3.factoryAddress"))

	addresses := []ethcommon.Address{
		factoryAddress,
	}

	addresses = append(addresses, uniswapv3.GetPoolAddresses(db)...)

	query := geth.FilterQuery{
		Addresses: addresses,
		FromBlock: big.NewInt(conf.GetInt64("fromBlock")),
		ToBlock:   big.NewInt(conf.GetInt64("toBlock")),
	}

	client, err := ethClientPool.RandClient(true)
	if err != nil {
		log.Fatal().Err(err).Msg("failed to find available client")
	}

	logsChan, _ := client.ChunkedFilterLogs(query, 0, conf.GetInt64("blockChunk"), conf.GetInt64("maxConnections"))

	start := time.Now()

	topics := map[string]kafkautils.Topic{
		"uniswapv3_factoryfeeamountenabled":                kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.factoryfeeamountenabled"), conf.GetString("kafka.env")),
		"uniswapv3_factoryownerchanged":                    kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.factoryownerchanged"), conf.GetString("kafka.env")),
		"uniswapv3_factorypoolcreated":                     kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.factorypoolcreated"), conf.GetString("kafka.env")),
		"uniswapv3_poolburn":                               kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.poolburn"), conf.GetString("kafka.env")),
		"uniswapv3_poolcollect":                            kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.poolcollect"), conf.GetString("kafka.env")),
		"uniswapv3_poolcollectprotocol":                    kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.poolcollectprotocol"), conf.GetString("kafka.env")),
		"uniswapv3_poolflash":                              kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.poolflash"), conf.GetString("kafka.env")),
		"uniswapv3_poolincreaseobservationcardinalitynext": kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.poolincreaseobservationcardinalitynext"), conf.GetString("kafka.env")),
		"uniswapv3_poolinitialize":                         kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.poolinitialize"), conf.GetString("kafka.env")),
		"uniswapv3_poolmint":                               kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.poolmint"), conf.GetString("kafka.env")),
		"uniswapv3_poolsetfeeprotocol":                     kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.poolsetfeeprotocol"), conf.GetString("kafka.env")),
		"uniswapv3_poolswap":                               kafkautils.MustParseTopic(conf.GetString("uniswapv3.kafka.topic.poolswap"), conf.GetString("kafka.env")),
	}

	connector := uniswapv3.Uniswapv3Connector{
		Topics: topics,
	}

	// database write buffer
	buffer := make([]*kafkautils.Message, 0, conf.GetInt("dbBatch"))

	logToMessage := map[string]func(ethtypes.Log) *kafkautils.Message{
		conf.GetString("uniswapv3.poolAddress"): connector.PoolLogToMsg,
	}

	for evLog := range logsChan {
		status.CurrentBlock = evLog.BlockNumber
		msg := logToMessage[evLog.Address.Hex()](evLog)
		status.EventsProcessed[msg.Topic.Schema()]++

		buffer = append(buffer, msg)

		// Flush to db
		if len(buffer) == conf.GetInt("dbBatch") {
			if conf.GetBool("upsert") {
				errs := db.InsertBatchKafkaMessages(buffer)
				for _, err := range errs {
					log.Error().Err(err).
						Msg("InsertBatchKafkaMessages error")
					status.Errors = append(status.Errors, err)
				}
			} else {
				err = db.CopyFromKafkaMessages(context.Background(), "nakji.common.0_0_0.liquiditypool_change", buffer)
				if err != nil {
					log.Error().Err(err).
						Msg("CopyFromKafkaMessages error")
					status.Errors = append(status.Errors, err)
				}
			}

			log.Info().
				Dur("time", time.Since(start)).
				Interface("lastmsg", msg).
				//Interface("status", status).
				Msg("uniswapv3 backfill in progress")

			buffer = nil
		}
	}

	// Flush remaining buffer
	if conf.GetBool("upsert") {
		errs := db.InsertBatchKafkaMessages(buffer)
		for _, err := range errs {
			log.Error().Err(err).
				Msg("InsertBatchKafkaMessages error")
			status.Errors = append(status.Errors, err)
		}
	} else {
		err = db.CopyFromKafkaMessages(context.Background(), "nakji.common.0_0_0.liquiditypool_change", buffer)
		if err != nil {
			log.Error().Err(err).
				Msg("CopyFromKafkaMessages error")
			status.Errors = append(status.Errors, err)
		}
	}

	log.Info().
		Dur("totaltime", time.Since(start)).
		Interface("status", &status).
		Msg("uniswapv3 backfill completed")
}
