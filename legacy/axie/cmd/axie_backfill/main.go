// Code generated by connectorgen.
// go run ./connectors/source/axie/cmd/axie_backfill/main.go --fromBlock 11945500 --toBlock 11945502
package main

import (
	"context"
	"encoding/json"
	"math/big"
	"net/http"
	"net/url"
	"time"

	"blep.ai/data/chain/ethereum/ethclient"
	"blep.ai/data/config"
	"blep.ai/data/connectors/source/axie"
	"blep.ai/data/database"
	"github.com/nakji-network/connector/kafkautils"

	geth "github.com/ethereum/go-ethereum"
	ethcommon "github.com/ethereum/go-ethereum/common"
	ethtypes "github.com/ethereum/go-ethereum/core/types"
	"github.com/rs/zerolog/log"
	"github.com/spf13/pflag"

	_ "net/http/pprof"
)

type Status struct {
	Step            int
	EventsProcessed map[string]uint64
	CurrentBlock    uint64
	Errors          []error
	FromBlock       int64
	ToBlock         int64
}

func main() {
	// Memory profiling
	go http.ListenAndServe(":6060", nil)

	// Load config in here to support flags
	pflag.Int64("fromBlock", 10000835, "backfill from this block")
	pflag.Int64("toBlock", 12000000, "backfill to this block")
	pflag.Int64("blockChunk", 3000, "get 100 blocks at a time to prevent timeouts")
	pflag.Int64P("maxConnections", "c", 10, "max simultaneous connections")
	pflag.Bool("upsert", true, "uses batch instead of COPY FROM when existing entries might exist")
	pflag.Int("dbBatch", 3000, "batch db writes by x items")
	pflag.Int64("processPairConcurrency", 4096, "max goroutines for pairProcessLog")

	var conf = config.InitConfig()

	conf.SetDefault("axie.axienftAddress", "0xF5b0A3eFB8e8E4c201e2A935F110eAaF3FFEcb8d")
	conf.SetDefault("axie.axsAddress", "0xbb0e17ef65f82ab018d8edd776e8dd940327b28b")
	conf.SetDefault("axie.bridgeAddress", "0x1A2a1c938CE3eC39b6D47113c7955bAa9DD454F2")
	conf.SetDefault("axie.slpAddress", "0xCC8Fa225D80b9c7D42F96e9570156c65D6cAAa25")

	// get status of this long running status
	status := Status{
		FromBlock: conf.GetInt64("fromBlock"),
		ToBlock:   conf.GetInt64("toBlock"),
	}
	http.HandleFunc("/status", func(w http.ResponseWriter, req *http.Request) {
		js, err := json.Marshal(status)
		if err != nil {
			http.Error(w, err.Error(), http.StatusInternalServerError)
			return
		}
		w.Header().Set("Content-Type", "application/json")
		w.Write(js)
	})
	go http.ListenAndServe(":8080", nil)

	// Load Topic registry
	kafkautils.TopicTypeRegistry.Load(axie.TopicTypes)

	rpcUrls := []string{(&url.URL{
		Scheme: conf.GetString("ethereum.archival.scheme"),
		User:   url.UserPassword(conf.GetString("ethereum.archival.username"), conf.GetString("ethereum.archival.password")),
		Host:   conf.GetString("ethereum.archival.host"),
		Path:   conf.GetString("ethereum.archival.path"),
	}).String(),
		(&url.URL{ // hacky to get past wss error
			Scheme: "wss",
			User:   url.UserPassword(conf.GetString("ethereum.archival.username"), conf.GetString("ethereum.archival.password")),
			Host:   conf.GetString("ethereum.archival.host"),
			Path:   conf.GetString("ethereum.archival.path"),
		}).String()}

	ethClientPool, err := ethclient.DialPoolContext(context.Background(), rpcUrls)
	if err != nil {
		log.Fatal().Err(err).Msg("Ethereum RPC connection error")
	}

	// Init historical db
	db, err := database.New(conf.GetString("timescaledb.connection"))
	if err != nil {
		log.Fatal().Err(err).Str("dsn", conf.GetString("timescaledb.connection")).Msg("Timescaledb connection failed")
	}
	defer db.Close()

	axienftAddress := ethcommon.HexToAddress(conf.GetString("axie.axienftAddress"))
	axsAddress := ethcommon.HexToAddress(conf.GetString("axie.axsAddress"))
	bridgeAddress := ethcommon.HexToAddress(conf.GetString("axie.bridgeAddress"))
	slpAddress := ethcommon.HexToAddress(conf.GetString("axie.slpAddress"))

	addresses := []ethcommon.Address{
		axienftAddress,
		axsAddress,
		bridgeAddress,
		slpAddress,
	}

	query := geth.FilterQuery{
		Addresses: addresses,
		FromBlock: big.NewInt(conf.GetInt64("fromBlock")),
		ToBlock:   big.NewInt(conf.GetInt64("toBlock")),
	}

	client, err := ethClientPool.RandClient(true)
	if err != nil {
		log.Fatal().Err(err).Msg("failed to find available client")
	}

	logsChan, _ := client.ChunkedFilterLogs(query, 0, conf.GetInt64("blockChunk"), conf.GetInt64("maxConnections"))

	start := time.Now()

	topics := map[string]kafkautils.Topic{
		"axienft_approval":       kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.axienft_approval"), conf.GetString("kafka.env")),
		"axienft_approvalforall": kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.axienft_approvalforall"), conf.GetString("kafka.env")),
		"axienft_axieevolved":    kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.axienft_axieevolved"), conf.GetString("kafka.env")),
		"axienft_axierebirthed":  kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.axienft_axierebirthed"), conf.GetString("kafka.env")),
		"axienft_axieretired":    kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.axienft_axieretired"), conf.GetString("kafka.env")),
		"axienft_axiespawned":    kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.axienft_axiespawned"), conf.GetString("kafka.env")),
		"axienft_transfer":       kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.axienft_transfer"), conf.GetString("kafka.env")),
		"axs_approval":           kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.axs_approval"), conf.GetString("kafka.env")),
		"axs_transfer":           kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.axs_transfer"), conf.GetString("kafka.env")),
		"bridge_adminchanged":    kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.bridge_adminchanged"), conf.GetString("kafka.env")),
		"bridge_adminremoved":    kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.bridge_adminremoved"), conf.GetString("kafka.env")),
		"bridge_paused":          kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.bridge_paused"), conf.GetString("kafka.env")),
		"bridge_proxyupdated":    kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.bridge_proxyupdated"), conf.GetString("kafka.env")),
		"bridge_tokendeposited":  kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.bridge_tokendeposited"), conf.GetString("kafka.env")),
		"bridge_tokenwithdrew":   kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.bridge_tokenwithdrew"), conf.GetString("kafka.env")),
		"bridge_unpaused":        kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.bridge_unpaused"), conf.GetString("kafka.env")),
		"slp_adminchanged":       kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.slp_adminchanged"), conf.GetString("kafka.env")),
		"slp_adminremoved":       kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.slp_adminremoved"), conf.GetString("kafka.env")),
		"slp_approval":           kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.slp_approval"), conf.GetString("kafka.env")),
		"slp_minteradded":        kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.slp_minteradded"), conf.GetString("kafka.env")),
		"slp_minterremoved":      kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.slp_minterremoved"), conf.GetString("kafka.env")),
		"slp_transfer":           kafkautils.MustParseTopic(conf.GetString("axie.kafka.topic.slp_transfer"), conf.GetString("kafka.env")),
	}

	connector := axie.AxieConnector{
		Topics: topics,
	}

	// database write buffer
	buffer := make([]*kafkautils.Message, 0, conf.GetInt("dbBatch"))

	logToMessage := map[string]func(ethtypes.Log) *kafkautils.Message{
		conf.GetString("axie.axienftAddress"): connector.AxienftLogToMsg,
		conf.GetString("axie.axsAddress"):     connector.AxsLogToMsg,
		conf.GetString("axie.bridgeAddress"):  connector.BridgeLogToMsg,
		conf.GetString("axie.slpAddress"):     connector.SlpLogToMsg,
	}

	for evLog := range logsChan {
		status.CurrentBlock = evLog.BlockNumber
		msg := logToMessage[evLog.Address.Hex()](evLog)
		status.EventsProcessed[msg.Topic.Schema()]++

		buffer = append(buffer, msg)

		// Flush to db
		if len(buffer) == conf.GetInt("dbBatch") {
			if conf.GetBool("upsert") {
				errs := db.InsertBatchKafkaMessages(buffer)
				for _, err := range errs {
					log.Error().Err(err).
						Msg("InsertBatchKafkaMessages error")
					status.Errors = append(status.Errors, err)
				}
			} else {
				err = db.CopyFromKafkaMessages(context.Background(), "nakji.common.0_0_0.liquiditypool_change", buffer)
				if err != nil {
					log.Error().Err(err).
						Msg("CopyFromKafkaMessages error")
					status.Errors = append(status.Errors, err)
				}
			}

			log.Info().
				Dur("time", time.Since(start)).
				Interface("lastmsg", msg).
				//Interface("status", status).
				Msg("axie backfill in progress")

			buffer = nil
		}
	}

	// Flush remaining buffer
	if conf.GetBool("upsert") {
		errs := db.InsertBatchKafkaMessages(buffer)
		for _, err := range errs {
			log.Error().Err(err).
				Msg("InsertBatchKafkaMessages error")
			status.Errors = append(status.Errors, err)
		}
	} else {
		err = db.CopyFromKafkaMessages(context.Background(), "nakji.common.0_0_0.liquiditypool_change", buffer)
		if err != nil {
			log.Error().Err(err).
				Msg("CopyFromKafkaMessages error")
			status.Errors = append(status.Errors, err)
		}
	}

	log.Info().
		Dur("totaltime", time.Since(start)).
		Interface("status", &status).
		Msg("axie backfill completed")
}
