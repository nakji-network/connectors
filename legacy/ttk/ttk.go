// Code generated by connectorgen - Edit as necessary.
package ttk

import (
	"context"
	"fmt"
	"math/big"
	"runtime"
	"strings"
	"sync"

	"blep.ai/data/chain/ethereum/ethclient"
	"blep.ai/data/common"
	"blep.ai/data/connectors/source/ttk/ttk"
	"github.com/nakji-network/connector/kafkautils"

	geth "github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi"
	ethcommon "github.com/ethereum/go-ethereum/common"
	ethtypes "github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/event"
	"github.com/rs/zerolog/log"
	"golang.org/x/sync/semaphore"
)

const (
	Namespace      = "ttk"
	TokenNamespace = "ethereum"
)

type TtkConnector struct {
	KP         kafkautils.ProducerInterface
	Topics     map[string]kafkautils.Topic
	ClientPool ethclient.ETHClientPool
	addresses  map[string][]ethcommon.Address
	blockCache map[uint64]uint64
}

func NewConnector(
	kp kafkautils.ProducerInterface,
	addresses map[string][]ethcommon.Address,
	topics map[string]kafkautils.Topic,
	ethClientPool ethclient.ETHClientPool,

) *TtkConnector {
	var blockCache map[uint64]uint64

	return &TtkConnector{
		KP:         kp,
		Topics:     topics,
		ClientPool: ethClientPool,
		addresses:  addresses,
		blockCache: blockCache,
	}
}

func (c *TtkConnector) Start(ctx context.Context, backfillNumBlocks uint64) {
	// Mainly serves as Keepalive for websocket connection to RPC endpoint by subscribing to new heads
	unsubscribe := make(chan interface{})
	headers := c.ClientPool.ConsumeHeaders(unsubscribe)
	ttkLogs, sub0 := c.startListener(ctx, "ttk")

	sink := make(chan *kafkautils.Message, 10000)

	err := c.KP.EnableTransactions()
	if err != nil {
		log.Fatal().Err(err).Msg("Transaction was not enabled")
	}
	go c.KP.WriteAndCommitSink(sink)

	var once sync.Once
	for {
		select {
		case <-ctx.Done():
			log.Info().Msg("worker cancelled and shutting down")
			return
		case header := <-headers:
			log.Debug().
				Str("block", header.Number.String()).
				Uint64("ts", header.Time).
				Msg("header received")

			ethclient.CacheBlockTimestamp(header.Hash(), header.Time)
		case err := <-sub0.Err():
			log.Fatal().Err(err).Msg("Event listener failed")
			return
		case evLog := <-ttkLogs:
			if evLog.Removed {
				continue
			}

			go once.Do(func() {
				c.backfill(sink, evLog.BlockNumber, backfillNumBlocks, "ttk", c.TtkLogToMsg)
			})

			msg := c.TtkLogToMsg(evLog)

			if msg != nil {
				sink <- msg
			}
		}
	}
}

func (c *TtkConnector) startListener(ctx context.Context, contractName string) (chan ethtypes.Log, event.Subscription) {
	query := geth.FilterQuery{
		Addresses: c.addresses[contractName],
	}
	eventLogs := make(chan ethtypes.Log)
	sub, err := c.ClientPool.SubscribeFilterLogs(ctx, query, eventLogs)
	if err != nil {
		msg := fmt.Sprintf("%s contract listener failed", contractName)
		log.Fatal().Err(err).
			Interface("query", query).
			Msg(msg)
	}
	msg := fmt.Sprintf("%s contract listener live", contractName)
	log.Info().Interface("query", query).Msg(msg)

	return eventLogs, sub
}

func (c *TtkConnector) TtkLogToMsg(evLog ethtypes.Log) *kafkautils.Message {
	ts, err := c.ClientPool.GetLogTimestamp(evLog, c.blockCache)
	if err != nil {
		log.Error().Err(err).
			Interface("blockNumber", evLog.BlockNumber).
			Msg("GetLogTimetsamp error")
	}

	ttkAbi, err := abi.JSON(strings.NewReader(ttk.TtkABI))
	if err != nil {
		log.Fatal().Err(err).Msg("Failed to read Ttk ABI")
	}

	ev, err := ttkAbi.EventByID(evLog.Topics[0])
	if err != nil {
		log.Fatal().Err(err).Msg("Failed to find event")
	}

	if ev == nil {
		return nil
	}

	switch ev.Name {
	case "Approval":
		event := new(ttk.TtkApproval)
		if err := ethclient.UnpackLog(ttkAbi, event, "Approval", evLog); err != nil {
			log.Error().Err(err).Msg("Unpack event error")
			return nil
		}

		return &kafkautils.Message{
			Topic: c.Topics["ttk_approval"],
			Key:   kafkautils.NewKey(Namespace, event.Raw.Address.Hex()),
			ProtoMsg: &ttk.Approval{
				Ts:      common.UnixToTimestampPb(int64(ts * 1000)),
				Owner:   event.Owner.Bytes(),
				Spender: event.Spender.Bytes(),
				Value:   event.Value.Bytes(),
			},
		}
	case "Transfer":
		event := new(ttk.TtkTransfer)
		if err := ethclient.UnpackLog(ttkAbi, event, "Transfer", evLog); err != nil {
			log.Error().Err(err).Msg("Unpack event error")
			return nil
		}

		return &kafkautils.Message{
			Topic: c.Topics["ttk_transfer"],
			Key:   kafkautils.NewKey(Namespace, event.Raw.Address.Hex()),
			ProtoMsg: &ttk.Transfer{
				Ts:    common.UnixToTimestampPb(int64(ts * 1000)),
				From:  event.From.Bytes(),
				To:    event.To.Bytes(),
				Value: event.Value.Bytes(),
			},
		}
	}

	return nil
}

// Backfill last 100 blocks
func (c *TtkConnector) backfill(out chan<- *kafkautils.Message, latestBlockNumber, backfillNumBlocks uint64, contract string, logToMsg func(ethtypes.Log) *kafkautils.Message) {
	filterQuery := geth.FilterQuery{
		FromBlock: big.NewInt(int64(latestBlockNumber - backfillNumBlocks)),
		ToBlock:   big.NewInt(int64(latestBlockNumber)),
		Addresses: c.addresses[contract],
	}

	logchan := make(chan ethtypes.Log)
	errchan := make(chan error)

	go c.ClientPool.ChunkedFilterLogs(context.Background(), filterQuery, 100, 1, logchan, errchan)

	maxWorkers := runtime.GOMAXPROCS(0)
	sem := semaphore.NewWeighted(int64(maxWorkers))

	for {
		select {
		case err := <-errchan:
			log.Error().Err(err).Msg("Failed to complete backfill")
		case evLog := <-logchan:
			if err := sem.Acquire(context.Background(), 1); err != nil {
				log.Error().Err(err).Msg("Failed to acquire semaaphor")
			}

			go func(evLog ethtypes.Log) {
				defer sem.Release(1)

				// Writes to out chan
				msg := logToMsg(evLog)
				if msg != nil {
					out <- msg
				}
			}(evLog)
		}
	}
}
